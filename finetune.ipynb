{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0133d6-d906-455d-8599-18a4943a4c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'waifu-diffusion'...\n",
      "remote: Enumerating objects: 580, done.\u001b[K\n",
      "remote: Total 580 (delta 0), reused 0 (delta 0), pack-reused 580\u001b[K\n",
      "Receiving objects: 100% (580/580), 133.56 MiB | 4.44 MiB/s, done.\n",
      "Resolving deltas: 100% (230/230), done.\n",
      "/workspace/waifu-diffusion\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "--2022-09-18 15:38:54--  https://r2-public-worker.drysys.workers.dev/sd-v1-4-full-ema.ckpt\n",
      "Resolving r2-public-worker.drysys.workers.dev (r2-public-worker.drysys.workers.dev)... 104.21.50.238, 172.67.213.219, 2606:4700:3034::ac43:d5db, ...\n",
      "Connecting to r2-public-worker.drysys.workers.dev (r2-public-worker.drysys.workers.dev)|104.21.50.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7703807346 (7.2G) [application/octet-stream]\n",
      "Saving to: ‘sd-v1-4-full-ema.ckpt’\n",
      "\n",
      "sd-v1-4-full-ema.ck 100%[===================>]   7.17G  49.6MB/s    in 2m 24s  \n",
      "\n",
      "2022-09-18 15:41:19 (50.9 MB/s) - ‘sd-v1-4-full-ema.ckpt’ saved [7703807346/7703807346]\n",
      "\n",
      "--2022-09-18 15:41:19--  http://sd-finetune.vonk.workers.dev/pokemon_labelled_n_830.tgz\n",
      "Resolving sd-finetune.vonk.workers.dev (sd-finetune.vonk.workers.dev)... 172.67.194.133, 104.21.92.135, 2606:4700:3036::6815:5c87, ...\n",
      "Connecting to sd-finetune.vonk.workers.dev (sd-finetune.vonk.workers.dev)|172.67.194.133|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 136372224 (130M) [application/gzip]\n",
      "Saving to: ‘pokemon_labelled_n_830.tgz’\n",
      "\n",
      "pokemon_labelled_n_ 100%[===================>] 130.05M  9.28MB/s    in 6.9s    \n",
      "\n",
      "2022-09-18 15:41:27 (18.8 MB/s) - ‘pokemon_labelled_n_830.tgz’ saved [136372224/136372224]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/harubaru/waifu-diffusion.git\n",
    "%cd waifu-diffusion\n",
    "!pip install -r requirements.txt -q\n",
    "!wget https://r2-public-worker.drysys.workers.dev/sd-v1-4-full-ema.ckpt\n",
    "!wget sd-finetune.vonk.workers.dev/pokemon_labelled_n_830.tgz\n",
    "!tar -xf pokemon_labelled_n_830.tgz\n",
    "!mv sd-v1-4-full-ema.ckpt model.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7310faf7-5794-4d11-a203-026255be0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e2aa2df-a0f7-4a1c-936d-80a1c0f18490",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0ce20d1-7056-455c-891e-98f5286e28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf pokemon_labelled_n_830.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557165f5-0685-4287-a956-0b79f953b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "# format to string image_0001.jpg\n",
    "\n",
    "str = f\"image_{index:04d}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d132b5f-8e4d-4de2-b22a-d7b3c2eb2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset postprocessing:\n",
    "# Move images to /img\n",
    "# Make captions in /txt using the filename\n",
    "\n",
    "!mkdir processed_images/img\n",
    "!mkdir processed_images/txt\n",
    "\n",
    "import os\n",
    "\n",
    "count = 0\n",
    "for f in os.listdir(\"processed_images\"):\n",
    "    if f.endswith(\"png\"):\n",
    "        count += 1\n",
    "        os.rename(f\"processed_images/{f}\", f\"processed_images/img/image{count:03d}.png\")\n",
    "        with open(f\"processed_images/txt/image{count:03d}.txt\", \"w\") as f2:\n",
    "            f2.write(f.rsplit(\".\", 1)[0].replace(\"_\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4038a1d-d015-4234-8105-756d9361382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "in_dir = \"processed_images/img\"\n",
    "\n",
    "for file in os.listdir(in_dir):\n",
    "    if file.endswith(\".png\"):\n",
    "        im = Image.open(os.path.join(in_dir, file))\n",
    "        rgb_im = im.convert(\"RGB\")\n",
    "        rgb_im.save(os.path.join(in_dir, file[:-4] + \".jpg\"))\n",
    "        # delete the original .png file\n",
    "        os.remove(os.path.join(in_dir, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0debc084-4ce7-44ed-8ef5-1e1d37ba760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: You need to change:\n",
    "# - The following command should have --data_root pointing to the right data\n",
    "# - The following command should have --gpu=0, for 1 GPU, --gpu=0,1, for 2 GPUs, or --gpu 0,1,2,3, for 4 GPUs\n",
    "# - configs/stable-diffusion/v1-finetune-4gpu.yaml should be modified such that in the \"data\" section batch_size and num_workers are set to the number of GPUs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b54202a5-083c-453d-ba48-6ed282ede0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img  txt\n"
     ]
    }
   ],
   "source": [
    "!ls ./processed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5d200-85b7-43c8-ad44-18732fc55e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 25\n",
      "Running on GPUs 0,\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loggers/test_tube.py:106: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.\n",
      "  \"The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the\"\n",
      "Monitoring val/loss_simple_ema as checkpoint metric.\n",
      "Merged modelckpt-cfg: \n",
      "{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2022-09-18T17-44-13_aesthetic/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3, 'every_n_train_steps': 500}}\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:288: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.\n",
      "  f\"Passing `Trainer(accelerator={accelerator!r})` has been deprecated\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:52: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v1.7. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "  \"Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  category=PossibleUserWarning,\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "{'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 1, 'num_workers': 1, 'wrap': False, 'train': {'target': 'ldm.data.local.LocalBase', 'params': {'size': 512, 'mode': 'train'}}, 'validation': {'target': 'ldm.data.local.LocalBase', 'params': {'size': 512, 'mode': 'val', 'val_split': 64}}}}\n",
      "<main.DataModuleFromConfig object at 0x7fa7d5e92a50>\n",
      "Fetching data.\n",
      "Constructing image-caption map.\n",
      "image-caption map has 830 examples\n",
      "Fetching data.\n",
      "Constructing image-caption map.\n",
      "image-caption map has 12 examples\n",
      "Fetching data.\n",
      "Constructing image-caption map.\n",
      "image-caption map has 830 examples\n",
      "Fetching data.\n",
      "Constructing image-caption map.\n",
      "image-caption map has 12 examples\n",
      "#### Data #####\n",
      "train, LocalBase, 830\n",
      "validation, LocalBase, 12\n",
      "accumulate_grad_batches = 1\n",
      "++++ NOT USING LR SCALING ++++\n",
      "Setting learning rate to 5.00e-06\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:808: LightningDeprecationWarning: `trainer.resume_from_checkpoint` is deprecated in v1.5 and will be removed in v2.0. Specify the fit checkpoint path with `trainer.fit(ckpt_path=)` instead.\n",
      "  ckpt_path = ckpt_path or self.resume_from_checkpoint\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:327: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  f\"Base `LightningModule.{hook}` hook signature has changed in v1.5.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:336: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "  \"The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:392: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "  f\"The `Callback.{hook}` hook has been deprecated in v1.6 and\"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:343: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  f\"Base `Callback.{hook}` hook signature has changed in v1.5.\"\n",
      "Fetching data.\n",
      "Constructing image-caption map.\n",
      "image-caption map has 830 examples\n",
      "Fetching data.\n",
      "Constructing image-caption map.\n",
      "image-caption map has 12 examples\n",
      "Global seed set to 25\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Fetching data.\n",
      "Constructing image-caption map.\n",
      "image-caption map has 830 examples\n",
      "Fetching data.\n",
      "Constructing image-caption map.\n",
      "image-caption map has 12 examples\n",
      "Restoring states from the checkpoint path at ../pokediffusion_epoch_3.ckpt\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/workspace/waifu-diffusion/logs/2022-09-18T17-07-00_aesthetic/checkpoints' to '/workspace/waifu-diffusion/logs/2022-09-18T17-44-13_aesthetic/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "  f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Setting up LambdaLR scheduler...\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper   | 859 M \n",
      "1 | model_ema         | LitEma             | 0     \n",
      "2 | first_stage_model | AutoencoderKL      | 83.7 M\n",
      "3 | cond_stage_model  | FrozenCLIPEmbedder | 123 M \n",
      "---------------------------------------------------------\n",
      "859 M     Trainable params\n",
      "206 M     Non-trainable params\n",
      "1.1 B     Total params\n",
      "4,264.941 Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at ../pokediffusion_epoch_3.ckpt\n",
      "Project config\n",
      "model:\n",
      "  base_learning_rate: 5.0e-06\n",
      "  target: ldm.models.diffusion.ddpm.LatentDiffusion\n",
      "  params:\n",
      "    linear_start: 0.00085\n",
      "    linear_end: 0.012\n",
      "    num_timesteps_cond: 1\n",
      "    log_every_t: 200\n",
      "    timesteps: 1000\n",
      "    first_stage_key: image\n",
      "    cond_stage_key: caption\n",
      "    image_size: 64\n",
      "    channels: 4\n",
      "    cond_stage_trainable: false\n",
      "    conditioning_key: crossattn\n",
      "    monitor: val/loss_simple_ema\n",
      "    scale_factor: 0.18215\n",
      "    scheduler_config:\n",
      "      target: ldm.lr_scheduler.LambdaLinearScheduler\n",
      "      params:\n",
      "        warm_up_steps:\n",
      "        - 1\n",
      "        cycle_lengths:\n",
      "        - 10000000000000\n",
      "        f_start:\n",
      "        - 1.0e-06\n",
      "        f_max:\n",
      "        - 1.0\n",
      "        f_min:\n",
      "        - 1.0\n",
      "    unet_config:\n",
      "      target: ldm.modules.diffusionmodules.openaimodel.UNetModel\n",
      "      params:\n",
      "        image_size: 32\n",
      "        in_channels: 4\n",
      "        out_channels: 4\n",
      "        model_channels: 320\n",
      "        attention_resolutions:\n",
      "        - 4\n",
      "        - 2\n",
      "        - 1\n",
      "        num_res_blocks: 2\n",
      "        channel_mult:\n",
      "        - 1\n",
      "        - 2\n",
      "        - 4\n",
      "        - 4\n",
      "        num_heads: 8\n",
      "        use_spatial_transformer: true\n",
      "        transformer_depth: 1\n",
      "        context_dim: 768\n",
      "        use_checkpoint: true\n",
      "        legacy: false\n",
      "    first_stage_config:\n",
      "      target: ldm.models.autoencoder.AutoencoderKL\n",
      "      params:\n",
      "        embed_dim: 4\n",
      "        monitor: val/rec_loss\n",
      "        ddconfig:\n",
      "          double_z: true\n",
      "          z_channels: 4\n",
      "          resolution: 512\n",
      "          in_channels: 3\n",
      "          out_ch: 3\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 2\n",
      "          - 4\n",
      "          - 4\n",
      "          num_res_blocks: 2\n",
      "          attn_resolutions: []\n",
      "          dropout: 0.0\n",
      "        lossconfig:\n",
      "          target: torch.nn.Identity\n",
      "    cond_stage_config:\n",
      "      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder\n",
      "data:\n",
      "  target: main.DataModuleFromConfig\n",
      "  params:\n",
      "    batch_size: 1\n",
      "    num_workers: 1\n",
      "    wrap: false\n",
      "    train:\n",
      "      target: ldm.data.local.LocalBase\n",
      "      params:\n",
      "        size: 512\n",
      "        mode: train\n",
      "    validation:\n",
      "      target: ldm.data.local.LocalBase\n",
      "      params:\n",
      "        size: 512\n",
      "        mode: val\n",
      "        val_split: 64\n",
      "trainer:\n",
      "  benchmark: true\n",
      "  val_check_interval: 5000000\n",
      "  num_sanity_val_steps: 0\n",
      "  accumulate_grad_batches: 1\n",
      "--data_root: null\n",
      "? ''\n",
      ": /processed_images: null\n",
      "\n",
      "Lightning config\n",
      "modelcheckpoint:\n",
      "  params:\n",
      "    every_n_train_steps: 500\n",
      "callbacks:\n",
      "  image_logger:\n",
      "    target: main.ImageLogger\n",
      "    params:\n",
      "      batch_frequency: 500\n",
      "      max_images: 4\n",
      "      increase_log_steps: false\n",
      "      log_first_step: false\n",
      "      log_images_kwargs:\n",
      "        use_ema_scope: false\n",
      "        inpaint: false\n",
      "        plot_progressive_rows: false\n",
      "        plot_diffusion_rows: false\n",
      "        'N': 4\n",
      "        ddim_steps: 50\n",
      "trainer:\n",
      "  accelerator: ddp\n",
      "  gpus: 0,\n",
      "  resume_from_checkpoint: ../pokediffusion_epoch_3.ckpt\n",
      "\n",
      "Sanity Checking: 0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s]Plotting: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:00<00:04, 11.99it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:00<00:03, 11.89it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:00<00:03, 12.05it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:00<00:03, 12.14it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:00<00:03, 12.29it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:00<00:03, 12.44it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:01<00:02, 12.53it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:01<00:02, 12.60it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:01<00:02, 11.88it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:01<00:02, 11.31it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:01<00:02, 11.61it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:01<00:02, 11.93it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:02<00:01, 12.17it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:02<00:01, 12.35it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:02<00:01, 12.47it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:02<00:01, 12.55it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:02<00:01, 12.60it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:02<00:01, 12.65it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:03<00:00, 12.66it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:03<00:00, 12.68it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:03<00:00, 12.69it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:03<00:00, 12.70it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:03<00:00, 12.68it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:03<00:00, 12.70it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:04<00:00, 12.38it/s]\u001b[A\n",
      "Plotting: Restored training weights\n",
      "Sanity Checking DataLoader 0:  50%|███████▌       | 1/2 [00:14<00:14, 14.26s/it]pop from empty list\n",
      "Plotting: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (1, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                            | 2/50 [00:00<00:03, 12.70it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▍                           | 4/50 [00:00<00:03, 12.36it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▌                          | 6/50 [00:00<00:03, 12.53it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▊                         | 8/50 [00:00<00:03, 12.51it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▊                       | 10/50 [00:00<00:03, 12.52it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▉                      | 12/50 [00:00<00:03, 12.60it/s]\u001b[A\n",
      "DDIM Sampler:  28%|████████                     | 14/50 [00:01<00:02, 12.63it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████▎                   | 16/50 [00:01<00:02, 12.66it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▍                  | 18/50 [00:01<00:02, 12.68it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▌                 | 20/50 [00:01<00:02, 12.69it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▊                | 22/50 [00:01<00:02, 12.69it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▉               | 24/50 [00:01<00:02, 12.70it/s]\u001b[A\n",
      "DDIM Sampler:  52%|███████████████              | 26/50 [00:02<00:01, 12.56it/s]\u001b[A\n",
      "DDIM Sampler:  56%|████████████████▏            | 28/50 [00:02<00:01, 12.62it/s]\u001b[A\n",
      "DDIM Sampler:  60%|█████████████████▍           | 30/50 [00:02<00:01, 12.65it/s]\u001b[A\n",
      "DDIM Sampler:  64%|██████████████████▌          | 32/50 [00:02<00:01, 12.67it/s]\u001b[A\n",
      "DDIM Sampler:  68%|███████████████████▋         | 34/50 [00:02<00:01, 12.69it/s]\u001b[A\n",
      "DDIM Sampler:  72%|████████████████████▉        | 36/50 [00:02<00:01, 12.66it/s]\u001b[A\n",
      "DDIM Sampler:  76%|██████████████████████       | 38/50 [00:03<00:00, 12.68it/s]\u001b[A\n",
      "DDIM Sampler:  80%|███████████████████████▏     | 40/50 [00:03<00:00, 12.69it/s]\u001b[A\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 42/50 [00:03<00:00, 12.63it/s]\u001b[A\n",
      "DDIM Sampler:  88%|█████████████████████████▌   | 44/50 [00:03<00:00, 12.64it/s]\u001b[A\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 46/50 [00:03<00:00, 12.66it/s]\u001b[A\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 48/50 [00:03<00:00, 12.65it/s]\u001b[A\n",
      "DDIM Sampler: 100%|█████████████████████████████| 50/50 [00:03<00:00, 12.64it/s]\u001b[A\n",
      "Plotting: Restored training weights\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "Training: 0it [00:00, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:2097: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  \"`Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. \"\n",
      "/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:2097: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  \"`Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. \"\n",
      "Epoch 9:   0%|                                          | 0/842 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n",
      "[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 9:  48%|▍| 407/842 [04:28<04:47,  1.52it/s, loss=0.0627, v_num=0, train/lo"
     ]
    }
   ],
   "source": [
    "!sh train.sh -t -n \"aesthetic\" --resume_from_checkpoint ../pokediffusion_epoch_3.ckpt --base ./configs/stable-diffusion/v1-finetune-4gpu.yaml --no-test --seed 25 --scale_lr False --data_root \"./processed_images\" --gpu=0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "737cf401-c7fe-4b99-bc6a-d57faad019d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo\n"
     ]
    }
   ],
   "source": [
    "print(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b5a93e-55e7-4ee7-bd0b-e9ee12743a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
